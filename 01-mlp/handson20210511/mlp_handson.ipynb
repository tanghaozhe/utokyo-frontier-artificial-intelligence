{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp_handson.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"13Zv9_PqQdSI"},"source":["# Multi Layer Perceptron (MLP) の実装"]},{"cell_type":"markdown","metadata":{"id":"jqkJ33OpQdSJ"},"source":["# 目次\n","\n","1. [全体像](#pipeline)\n","1. [目標](#goal)\n","- [下準備](#prepare)\n","- [活性化関数](#activate)\n","- [線形層](#linear)\n","- [多層パーセプトロン](#mlp)\n","- [学習](#train)"]},{"cell_type":"markdown","metadata":{"id":"1i-R_K4aQdSL"},"source":["# 1. 全体像<a class=\"anchor\" id=\"pipeline\"></a>\n","- 変数のあとの括弧は，行列の形状 (numpy.ndarrayのshape) を表す．\n","- Nはバッチサイズ\n","\n","※ バッチ処理とは……画像を1枚ずつ処理するのではなく，複数枚まとめて処理すること．画像1枚は，28×28のサイズから(784, )の形状をもつ1次元配列になるようにリサイズされているが，これをN枚まとめて(N, 784)の2次元配列として扱う．N枚まとめて行列演算を実行することで，1枚ずつ処理するより高速に処理できるようになる．"]},{"cell_type":"markdown","metadata":{"id":"Ym-PX-LSHzKQ"},"source":["<img src=\"https://drive.google.com/uc?export=download&id=1q-o-iqPQ5gdDajty-sFNEJKYWRRXicW5\">"]},{"cell_type":"markdown","metadata":{"id":"j4prYVN-QdSN"},"source":["## 2. 目標<a class=\"anchor\" id=\"goal\"></a>\n","- 多層 (例では3層) のニューラルネットワークを構築する．\n","- まず**活性化関数**をそれぞれ実装し，次に**線形層**の実装を行い，最後にそれらをまとめて**多層パーセプトロン**の実装を行う．\n","- 最終的には，下のように各レイヤーの入出力のユニット数と活性化関数を指定するだけでモデルが構築できるようにする．\n","- 例：3層・隠れ層のユニット数が1000．活性化関数はReLUを用いる場合\n","```python\n","model = MLP([Linear(784, 1000, ReLU),\n","                        Linear(1000, 1000, ReLU),\n","                        Linear(1000, 10, Softmax)])\n","```\n","- こうすると，4層以上への拡張や，活性化関数の変更などがしやすい"]},{"cell_type":"markdown","metadata":{"id":"0JS2J7NlQdSO"},"source":["## 3. 下準備<a class=\"anchor\" id=\"prepare\"></a>"]},{"cell_type":"markdown","metadata":{"id":"cHmydwnpQdSO"},"source":["### ライブラリのインポート"]},{"cell_type":"markdown","metadata":{"id":"XII_cLmkQdSP"},"source":["- matplotlib: 図やグラフの描画など．\n","- numpy: 行列演算など\n","- sklearn: scikit-learn．様々な機械学習のモデルが利用できるが，今回はMNISTのデータをダウンロードするのに用いる．"]},{"cell_type":"code","metadata":{"id":"9YnjWwdAHzKU"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from __future__ import unicode_literals\n","from __future__ import print_function\n","from __future__ import division\n","from __future__ import absolute_import"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naAKnEAfQ80X"},"source":["from google.colab import drive # driveを接続\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERDFaEjvINmp"},"source":["# drive中の課題ファイルのあるディレクトリに移動\n","%cd /content/gdrive/My Drive/handson20210511/ \n","\n","from test_mlp import *  # テスト用"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-VNIETHcQdST"},"source":["### MNISTデータの読み込み"]},{"cell_type":"markdown","metadata":{"id":"2FGDjTIMQdSU"},"source":["- データをダウンロードする．一度ダウンロードすると，その後はデータを参照して読み込んでくれるので，毎回ダウンロードしなくても良くなる．\n","- X：画像データ(各画像784次元）， Y：ラベル\n","- mnistのデータは，0~255のint型で表されているが，これを**255で割って正規化**する．"]},{"cell_type":"code","metadata":{"id":"6BZJ3zTDQdSV"},"source":["X, Y = fetch_openml('mnist_784', version=1, data_home=\"./data/\", return_X_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHscB6lSQdSX"},"source":["X = X / 255.\n","Y = Y.astype(\"int\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bwuJYGVDQdSZ"},"source":["### データセットの可視化"]},{"cell_type":"code","metadata":{"id":"gEf1SMSNQdSa"},"source":["for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    plt.imshow(X[i * 6500].reshape(28, 28), cmap='gray_r')\n","    plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoeLXRkBQdSd"},"source":["### 訓練・テストデータに分割"]},{"cell_type":"markdown","metadata":{"id":"lyT_cKgnQdSe"},"source":["- 訓練データで学習し，同じ訓練データで性能の評価を行うと，訓練データでは良い性能を示すが，データを少しでも変えると性能が低下してしまうことがある（**過学習**）．\n","- <span style=\"text-decoration: underline\">よって，学習する訓練データとは異なるテストデータで性能評価を行う</span>．"]},{"cell_type":"code","metadata":{"id":"2TqTX6ViQdSf"},"source":["train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=2)\n","train_y = np.eye(10)[train_y].astype(np.int32)\n","test_y = np.eye(10)[test_y].astype(np.int32)\n","train_n = train_x.shape[0]\n","test_n = test_x.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BB3F7Kt8QdSo"},"source":["## 4. 活性化関数の実装<a class=\"anchor\" id=\"activate\"></a>"]},{"cell_type":"markdown","metadata":{"id":"tGt8wTodQdSq"},"source":["- ここでは，活性化関数として広く知られる Sigmoid関数と，ReLU関数，及び出力層の活性化関数であるSoftmax関数の実装を行う．\n","- 各関数の詳細については，講義スライドを参照"]},{"cell_type":"markdown","metadata":{"id":"ol4lEIZuQdSr"},"source":["### <font color=\"crimson\">課題</font>：Sigmoid関数の実装"]},{"cell_type":"markdown","metadata":{"id":"ABnSViAHQdSt"},"source":["- 順伝播計算\n","\n","    - $h(x) = \\sigma(x) = \\dfrac{1}{1+e^{-x}}$\n","\n","- 逆伝播計算\n","\n","    - $h'(x) = \\sigma'(x) = \\sigma(x)\\cdot(1-\\sigma(x))$\n","\n","※ 順伝播の $\\sigma(x)$ と，逆伝播の $\\sigma(x)$ は同じなので，2回計算しなくてもOK"]},{"cell_type":"markdown","metadata":{"id":"ezjZetRAQdSu"},"source":["<details>\n","    <summary>ヒント</summary>\n","    <div>\n","        <br>\n","        - np.exp(x) を用いて\n","  $\\exp(x)$を計算できる\n","        <br>\n","        - 順伝播の計算結果は， self.y に保存されているので，逆伝播計算ではそれを使おう\n","    </div>\n","</details>"]},{"cell_type":"code","metadata":{"id":"LQgU4_SBQdSv"},"source":["class Sigmoid:\n","    def __init__(self):\n","        self.y = None\n","        \n","    def __call__(self, x):\n","        y =   # 順伝播計算\n","        self.y = y\n","        return y\n","    \n","    def backward(self):\n","        return   # 逆伝播計算"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYUIf9IXQdSz"},"source":["test_sigmoid(Sigmoid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1aXj3-i6QdS3"},"source":["### ReLU関数"]},{"cell_type":"markdown","metadata":{"id":"8J0o5YWSQdS4"},"source":["- 順伝播\n","    - $h(x) = \\max(0, x)$\n","- 逆伝播\n","  - $h'(x) =\n","  \\left\\{ \\begin{array}{ll}\n","  1 & (x > 0) \\\\\n","  0 & (x \\leq 0) \\\\\n","  \\end{array} \\right.$"]},{"cell_type":"markdown","metadata":{"id":"8T-6aCEJQdS6"},"source":["<details>\n","    <summary>ヒント</summary>\n","    <div>\n","        - ndarrayから，0より大きい要素をTrue, 0以下の要素をFalseとなるようなマスクを作成する\n","        <div>\n","        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n","        >>> a = np.array([[-1, 1], [0, 1]])\n","        >>> print(a)\n","        [[-1  1]\n","         [ 0  1]]\n","        >>> print(a > 0)\n","        [[False  True]\n","         [False  True]]\n","        >>> print(a * (a>0))\n","        [[0 1]\n","         [0 1]]\n","         </code></pre>\n","        </div>\n","    </div>\n","</details>"]},{"cell_type":"code","metadata":{"id":"U6_wTFdiQdS7"},"source":["class ReLU:\n","    def __init__(self):\n","        self.x = None\n","        \n","    def __call__(self, x):\n","        self.x = x\n","        return   # 順伝播計算\n","    \n","    def backward(self):\n","        return   # 逆伝播計算"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnZ5pNqoQdS9"},"source":["test_relu(ReLU)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"991-7gU-QdS_"},"source":["### Softmax関数\n","\n","- ロジスティック回帰と同様に実装する\n","\n","- 入力：$\\boldsymbol{X}=(\\boldsymbol{x_1},\\boldsymbol{x_2},\\cdots,\\boldsymbol{x_N})^T\\in\\mathbb{R}^{N\\times K}$（データ行列）\n","\n","\n","- 出力：$\\boldsymbol{Y}=(\\boldsymbol{y_1},\\boldsymbol{y_2},\\cdots,\\boldsymbol{y_N})^T\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = softmax(\\boldsymbol{x_n})_k$\n","\n","\n","- オーバーフローを防ぐために$\\boldsymbol{x}_n$の最大値を$\\boldsymbol{x}_n$自身から引く\n","\n","$$\n","\\begin{align}\n","softmax(\\boldsymbol{x})_k&= \\frac{\\exp (x_{k})} {\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (-x_{max})\\exp (x_{k})}{\\exp (-x_{max})\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (x_{k}-x_{max})} {\\Sigma_{i=1}^{K}{\\exp (x_{i}-x_{max})}}\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Kd0fV0JpQdTA"},"source":["<details>\n","    <summary>ヒント: 最大値の取得</summary>\n","    <div>\n","        - ndarrayから，最大値を取得したい\n","        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n","        >>> A = np.array([[1, 2], [3, 4], [5, 6]])\n","        >>> print(A.shape)\n","        (3, 2)\n","        >>> A.max()\n","        6\n","        </code></pre>\n","        - ここでは，N×Kの配列について，1次元目の要素ごとに計N個の最大値を取得したい\n","        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n","        >>> A.max(axis=1)\n","        array([2, 4, 6])\n","        </code></pre>\n","        - さらに，配列の形状はN×1にしておきたい\n","        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n","        >>> A.max(axis=1, keepdims=True)\n","        array([[2],\n","       [4],\n","       [6]])\n","        </code></pre>\n","    </div>\n","</details>"]},{"cell_type":"code","metadata":{"id":"Z8TQ9BpQQdTB"},"source":["class Softmax:\n","    def __init__(self):\n","        self.y = None\n","        \n","    def __call__(self, x):\n","        exp_x =   # ここで exp(x - x_max) を計算しよう\n","        y =  # exp_x を用いて softmax を計算しよう\n","        self.y = y\n","        return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdd4HC4XQdTE"},"source":["test_softmax(Softmax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aheob1svHzLO"},"source":["## 5. 線形層<a class=\"anchor\" id=\"linear\"></a>\n","- 引数\n","    - in_dim : 入力のユニット数\n","    - out_dim : 出力のユニット数\n","    - activation : 活性化関数\n","- 計算\n","    - 順伝播\n","        <div class=\"clearfix\">\n","        <img style=\"float: left;\" src=\"https://drive.google.com/uc?export=download&id=1cpmyxw8vSzLjwmMXgpWVTJrjrW3Nqzhh\" width=200px>\n","\n","        </div>\n","        - ヒント : np.dot(A, B)\n","    - 逆伝播\n","        - 入力 (dout)\n","            - 一つ上の層 (l+1層) からの出力\n","  $(\\boldsymbol{o}^{(l+1)})$\n","        - 誤差\n","        <div class=\"clearfix\">\n","        <img style=\"float: left;\" src=\"https://drive.google.com/uc?export=download&id=13fq5TC9KlwZ08qrpu5aYxPCL_XAT4AsZ\" width=200px>\n","        </div>\n","        \n","            - $\\odot$ は要素積\n","        - 勾配計算\n","        \n","        <div class=\"clearfix\">\n","        <img style=\"float: left;\" src=\"https://drive.google.com/uc?export=download&id=1_1FtLMvBMMgLJwKbuuRPWd_RLCFpo2p5\" width=200px>\n","        </div>"]},{"cell_type":"markdown","metadata":{"id":"2TbjOmbYQdTH"},"source":["<details>\n","    <summary>ヒント: 行列積の計算について</summary>\n","    <br>\n","    <div>\n","        \n","$A = \\left(\n","    \\begin{array}{ccc}\n","      0 & 1 & 2 \\\\\n","      1 & 2 & 3\n","    \\end{array}\n","  \\right)\n","$ , \n","$\n","B = \\left(\n","\\begin{array}{cc}\n","  0 & 1\\\\\n","  1 & 2 \\\\\n","  2 & 3\n","\\end{array}\n","\\right)\n","$ としたとき， $C = AB$ を計算する例\n","        <br>\n","        <div>\n","        <div>\n","        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n","        >>> A = np.array([[0, 1, 2], [1, 2, 3]])\n","        >>> B = np.array([[0, 1], [1, 2], [2, 3]])\n","        >>> print(A.shape, B.shape)\n","        (2, 3) (3, 2)\n","        >>> print(a > 0)\n","        [[False  True]\n","         [False  True]]\n","        >>> C = np.dot(A, B)\n","        >>> print(C)\n","        [[ 5  8]\n","         [ 8 14]]\n","         </code></pre>\n","          </div>\n","        </div>\n","    </div>\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"8-1axFyoQdTI"},"source":["<details>\n","    <summary>ヒント: 行列の要素積</summary>\n","    <br>\n","    <div>\n","        <div>\n","        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n","        >>> A = np.array([[0, 1], [2, 3]])\n","        >>> B = np.array([[1, 2], [3, 4]])\n","        >>> print(A*B)\n","        [[ 0  2]\n","         [ 6 12]]\n","         </code></pre>\n","        </div>\n","    </div>\n","</details>"]},{"cell_type":"code","metadata":{"id":"a0d8WJG-QdTJ"},"source":["class Linear:\n","    def __init__(self, in_dim, out_dim, activation):\n","        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim))\n","        self.b = np.zeros(out_dim)\n","        self.activation = activation()\n","        self.delta = None\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","    def __call__(self, x):\n","        # 順伝播計算\n","        self.x = x\n","        u =   # self.W, self.b, x を用いて u を計算しよう\n","        self.z = self.activation(u)\n","        return self.z\n","    \n","    def backward(self, dout):\n","        # 誤差計算\n","        self.delta =  # dout と活性化関数の逆伝播 (self.activation.backward()) を用いて delta を計算しよう\n","        dout =  # self.delta, self.W を用いて 出力 o を計算しよう\n","        \n","        # 勾配計算\n","        self.dW =   # dW を計算しよう\n","        self.db =  # db を計算しよう\n","        \n","        return dout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Aqw9yzXQdTN"},"source":["test_linear(Linear, ReLU)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PK-soITrQdTS"},"source":["## 6. 多層パーセプトロンの実装<a class=\"anchor\" id=\"mlp\"></a>\n","- 今までに実装してきた活性化関数，Linear層を組み合わせて，多層のパーセプトロンを実装する．\n","- ここでは，先に実装したLinear層を組み合わせ，全体を通した**順伝播計算**と，**損失の計算**，**誤差逆伝播計算**，及び**重み・バイアスの更新**を実装する．\n","- 例：3層・隠れ層のユニット数が1000．活性化関数はReLUを用いる場合\n","```python\n","model = MLP([Linear(784, 1000, ReLU),\n","                        Linear(1000, 1000, ReLU),\n","                        Linear(1000, 10, Softmax)])\n","```\n","- **lr** : 学習率 (learning rate)．学習率とは，重み・バイアスの更新量を決定するハイパーパラメータ．つまり，パラメータ更新量 = 学習率 × 現在の勾配"]},{"cell_type":"markdown","metadata":{"id":"bYqEi8TbHzLW"},"source":["### 全体像の再掲\n","<img src=\"https://drive.google.com/uc?export=download&id=11PBGMeAYpaeOtFH43FpEnrwZPbn8XbDf\">"]},{"cell_type":"markdown","metadata":{"id":"ucD2gz43QdTV"},"source":["### <font color=\"crimson\">課題</font> : 多層パーセプトロン"]},{"cell_type":"code","metadata":{"id":"QJMFMxeiQdTW"},"source":["class MLP():\n","    def __init__(self, layers):\n","        self.layers = layers\n","        \n","    def train(self, x, t, lr):     \n","        # 1. 順伝播\n","        self.y = x\n","        for layer in self.layers:\n","            self.y =   # 順伝播計算を順番に行い， 出力 y を計算しよう\n","        \n","        # 2. 損失関数の計算\n","        self.loss = np.sum(-t*np.log(self.y + 1e-7)) / len(x)\n","        \n","        # 3. 誤差逆伝播\n","        # 3.1. 最終層\n","        # 3.1.1. 最終層の誤差・勾配計算\n","        batchsize = len(self.layers[-1].x)\n","        delta = (self.y - t) / batchsize\n","        self.layers[-1].delta = delta\n","        self.layers[-1].dW = np.dot(self.layers[-1].x.T, self.layers[-1].delta)\n","        self.layers[-1].db = np.dot(np.ones(batchsize), self.layers[-1].delta)\n","        dout = np.dot(self.layers[-1].delta, self.layers[-1].W.T)\n","        \n","        # 3.1.2. 最終層のパラメータ更新\n","        self.layers[-1].W -=  # self.layers[-1].dW を用いて最終層の重みを更新しよう\n","        self.layers[-1].b -=   # self.layers[-1].db を用いて最終層のバイアスを更新しよう\n","        \n","        # 3.2. 中間層\n","        for layer in self.layers[-2::-1]:\n","            # 3.2.1. 中間層の誤差・勾配計算\n","            dout =   # 逆伝播計算を順番に実行しよう\n","            \n","            # 3.2.2. パラメータの更新\n","            layer.W -=   # 各層の重みを更新\n","            layer.b -=   # 各層のバイアスを更新\n","            \n","        return self.loss\n","\n","    def test(self, x, t):\n","        # 性能をテストデータで調べるために用いる\n","        # よって，誤差逆伝播は不要\n","        # 順伝播 (train関数と同様)\n","        self.y = x\n","        for layer in self.layers:\n","            self.y = layer(self.y)\n","        self.loss = np.sum(-t*np.log(self.y + 1e-7)) / len(x)\n","        return self.loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LcpDSNp5QdTY"},"source":["## モデルの構築\n","- ここでは，図に示してきたような3層のニューラルネットワークを構築する\n","- 活性化関数はSigmoid関数とし， 隠れ層のニューロン数はいずれも1000とする．"]},{"cell_type":"code","metadata":{"id":"PT0jQBAqQdTZ"},"source":["model = MLP([Linear(784, 1000, Sigmoid),\n","                        Linear(1000, 1000, Sigmoid),\n","                        Linear(1000, 10, Softmax)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NhRhi9LlQdTd"},"source":["# 7. 学習<a class=\"anchor\" id=\"train\"></a>\n","\n","- n_epoch : エポック数．1エポックとは，学習時に訓練データをすべて学習した回数を表す．\n","- batchsize: バッチサイズ．\n","- lr: 学習率 (learning rate)．"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"lMfyQGcGQdTd"},"source":["n_epoch = 20\n","batchsize = 100\n","lr = 0.5\n","\n","for epoch in range(n_epoch):\n","    print('epoch %d | ' % epoch, end=\"\")\n","    \n","    # 訓練\n","    sum_loss = 0\n","    pred_y = []\n","    perm = np.random.permutation(train_n)\n","    \n","    for i in range(0, train_n, batchsize):\n","        x = train_x[perm[i: i+batchsize]]\n","        t = train_y[perm[i: i+batchsize]]\n","        sum_loss += model.train(x, t, lr) * len(x)\n","        # model.y には， (N, 10)の形で，画像が0~9の各数字のどれに分類されるかの事後確率が入っている\n","        # そこで，最も大きい値をもつインデックスを取得することで，識別結果を得ることができる\n","        pred_y.extend(np.argmax(model.y, axis=1))\n","    \n","    loss = sum_loss / train_n\n","    \n","    # accuracy : 予測結果を1-hot表現に変換し，正解との要素積の和を取ることで，正解数を計算できる．\n","    accuracy = np.sum(np.eye(10)[pred_y] * train_y[perm]) / train_n\n","    print('Train loss %.3f, accuracy %.4f | ' %(loss, accuracy), end=\"\")\n","    \n","    \n","    # テスト\n","    sum_loss = 0\n","    pred_y = []\n","    \n","    for i in range(0, test_n, batchsize):\n","        x = test_x[i: i+batchsize]\n","        t = test_y[i: i+batchsize]\n","        \n","        sum_loss += model.test(x, t) * len(x)\n","        pred_y.extend(np.argmax(model.y, axis=1))\n","\n","    loss = sum_loss / test_n\n","    accuracy = np.sum(np.eye(10)[pred_y] * test_y) / test_n\n","    print('Test loss %.3f, accuracy %.4f' %(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BW4C1UNhQdTf"},"source":["初期設定では，98%前後のaccuracyになったのではないかと思います．\n","\n","ニューラルネットワークの学習には，様々なハイパーパラメータ（学習率など）を上手く設定する必要があります．\n","\n","また，活性化関数や，重みの初期値の工夫，Dropoutなどのテクニックを用いることで，さらに性能が向上する可能性があります．\n","\n","モデルを工夫して，よりよい性能を発揮するニューラルネットワークを構築してみましょう．"]}]}