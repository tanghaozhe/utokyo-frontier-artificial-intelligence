{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"softmax_handson.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dTC4iCLP0sVG"},"source":["# ソフトマックス回帰　(softmax regression)の実装\n","# 目次\n","1. 概要\n","- 目標\n","- 下準備\n","- softmax関数の実装\n","- 多クラス交差エントロピー誤差の実装\n","- ソフトマックス回帰クラスの実装\n","- 学習\n","\n","# 1. 概要\n","- ロジスティック回帰の一般化で，多クラスに対応した手法\n","- K個のクラス識別問題を考える．$\\boldsymbol{x}$：入力データ，$\\boldsymbol{t}$：教師データ\n","\n","\\begin{align}\n","\\it{D}=\\left\\{\\left(\\boldsymbol{x}_i,\\boldsymbol{t}_i\\right)\\right\\}_{i=1}^{N}\\, ,\\boldsymbol{x}\\in\\mathbb{R}^d,\\, \\boldsymbol{t}\\in\\left\\{1,\\cdots,K\\right\\}\n","\\end{align}\n","\n","- 各クラスの事後確率を求める．各クラスごとに重み行列 $\\boldsymbol{w}^{(𝑗)}$を持つ．\n","$$\n","P(y=1|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(1)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\\\\\n","P(y=2|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(2)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\\\\\n","\\vdots\\\\\n","P(y=K|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(K)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"76H1sytv0sVI"},"source":["# 2. 目標\n","- ソフトマックス回帰を実装して，mnist（手書き数字データセット）を識別する．\n","- まず**活性化関数**の一種である**softmax**関数と，**交差エントロピー関数**を実装する．その後**確率的勾配降下法**を実装し，**SoftmaxRegression**クラスを実装する．"]},{"cell_type":"markdown","metadata":{"id":"AwatT0oB0sVJ"},"source":["# 3. 下準備\n","## 3.1 ライブラリのインポート\n","- matplotlib: 図やグラフの描画など．\n","- numpy: 行列演算など\n","- sklearn: scikit-learn．様々な機械学習のモデルが利用できるが，今回はMNISTのデータをダウンロードするのに用いる．"]},{"cell_type":"code","metadata":{"id":"PRi2I9olK7T1"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from __future__ import print_function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBn555zkVXAO"},"source":["from google.colab import drive # driveを接続\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7gHPHdZLVfT4"},"source":["# drive中の課題ファイルのあるディレクトリに移動\n","%cd /content/gdrive/My Drive/handson20210511/\n","\n","from test_softmax import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RPI09MhY0sVQ"},"source":["## 3.2 MNISTデータの読み込み\n"]},{"cell_type":"markdown","metadata":{"id":"UMrwgVFpxPwA"},"source":["- データをダウンロードする．一度ダウンロードすると，その後はデータを参照して読み込んでくれるので，毎回ダウンロードしなくても良くなる．\n","- X：画像データ(各画像784次元）， Y：ラベル\n","- mnistのデータは，0~255のint型で表されているが，これを**255で割って正規化**する．"]},{"cell_type":"code","metadata":{"id":"_VFl1sNe0sVR"},"source":["X, Y = fetch_openml('mnist_784', version=1, data_home=\".\", return_X_y=True)\n","X = X / 255.\n","Y = Y.astype(\"int\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4WlHWs10sVT"},"source":["### データセットの可視化"]},{"cell_type":"code","metadata":{"id":"SFtmq1PO0sVU"},"source":["for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    plt.imshow(X[i * 6500].reshape(28, 28), cmap='gray_r')\n","    plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-zZtlXz0sVW"},"source":["### 学習用・テスト用データの分割\n","- 訓練データで学習し，同じ訓練データで性能の評価を行うと，訓練データでは良い性能を示すが，データを少しでも変えると性能が低下してしまうことがある（**過学習**）．\n","- よって，学習する訓練データとは異なるテストデータで性能評価を行う．"]},{"cell_type":"code","metadata":{"id":"yNEB4-H60sVX"},"source":["train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=2)\n","train_y = np.eye(10)[train_y].astype(np.int32)\n","test_y = np.eye(10)[test_y].astype(np.int32)\n","train_n = train_x.shape[0]\n","test_n = test_x.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkr-Eal00sVb"},"source":["# 4. softmax関数の実装\n","活性化関数の一種であるsoftmax関数を実装する．\n","- 関数：softmax\n","  - 入力：$\\boldsymbol{X}=(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_N)^\\top\\in\\mathbb{R}^{N\\times K}$\n","  - 出力：$\\boldsymbol{Y}=(\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\cdots,\\boldsymbol{y}_N)^\\top\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = softmax(\\boldsymbol{x}_n)_k$\n","  - オーバーフローを防ぐために$\\boldsymbol{x}_n$の最大値を$\\boldsymbol{x}_n$自身から引く\n","$$\n","\\begin{align}\n","softmax(\\boldsymbol{x})_k&= \\frac{\\exp (x_{k})} {\\Sigma_{i=1}^{K}{\\exp (x_{i})}}\\\\\n","&=\\frac{\\exp (-x_{max})\\exp (x_{k})}{\\exp (-x_{max})\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (x_{k}-x_{max})} {\\Sigma_{i=1}^{K}{\\exp (x_{i}-x_{max})}}\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"1zrpt_q00sVc"},"source":["<details>\n","<summary>\n","ヒント\n","</summary>\n","<ol>\n","    <li>最大値\n","    <ul> \n","        <li>\n","\n","```axis=1```を指定するとデータ\n","  $\\boldsymbol{x_n}$ごとに最大値を計算できる</li>\n","        <li>行列のshapeを変えたくない場合は，```keepdims=True```を指定する</li>\n","    </ul></li>\n","    <li>$\\exp$\n","    <ul>\n","    <li>```np.exp()```</li>\n","    </ul></li>\n","    <li>合計\n","    <ul>\n","    <li>```np.sum()```</li>\n","    <li>```axis=1```を指定するとデータ$\\boldsymbol{x_n}$ごとに合計を計算できる</li>\n","    <li>行列のshapeを変えたくない場合は，```keepdims=True```を指定する</li>\n","    </ul></li>\n","</ol>\n","</details>"]},{"cell_type":"code","metadata":{"id":"FbcZVE610sVc"},"source":["def softmax(x):\n","    # TODO\n","    y = \n","    # TODO\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-vUNsC50sVg"},"source":["テスト．以下のセルを実行"]},{"cell_type":"code","metadata":{"id":"aZ5nQ-Se0sVg"},"source":["test_softmax(softmax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2NRmZt70sVj"},"source":["# 5. 多クラスの交差エントロピー誤差の実装\n","- 関数：cross_entropy\n","    - 入力： $Y=(\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\cdots,\\boldsymbol{y}_N)^\\top\\in \\mathbb{R}^{N\\times K}$, \n","$T=(\\boldsymbol{t}_1,\\boldsymbol{t}_2,\\cdots,\\boldsymbol{t}_N)^\\top\\in \\mathbb{R}^{N\\times K}$<br />\n","$\\boldsymbol{y}_n$はソフトマックス関数の出力，$\\boldsymbol{t}_n$は教師ラベル(1-of-K表現)\n","\n","    - 出力： \n","    $$L=-\\frac{1}{N}\\sum_{n=1}^N \\sum_i \\boldsymbol t_{n,i} \\log \\boldsymbol y_{n,i}\\in\\mathbb{R}^1\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"ERm53VeX0sVk"},"source":["<details>\n","<summary>\n","ヒント\n","</summary>\n","<ol>\n","    <li>\n","\n","$\\log$\n","    <ul> \n","        <li>```np.log()```</li>\n","    </ul></li>\n","    <li>合計\n","    <ul>\n","    <li>```np.sum()```</li>\n","    </ul></li>\n","    <li>平均\n","    <ul>\n","    <li>```np.mean()```</li>\n","    </ul></li>\n","</ol>\n","</details>"]},{"cell_type":"code","metadata":{"id":"ObGdYiNQ0sVl"},"source":["def cross_entropy(y, t):\n","    # TODO\n","    L = \n","    # TODO\n","    return L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWvg50d10sVo"},"source":["テスト．以下のセルを実行"]},{"cell_type":"code","metadata":{"id":"HXiQGpZL0sVp"},"source":["test_cross_entropy(cross_entropy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsq6sAK40sVr"},"source":["# 6. ソフトマックス回帰の実装\n","ソフトマックス回帰クラスを実装する．\n","## 6.1 勾配降下法の実装\n","SoftmaxRegressionクラスのgradient_decent関数を実装してください．\n","- 関数：gradient_descent  \n","    - 入力：\n","        - 学習データ： $\\boldsymbol{X}\\in\\mathbb{R}^{N\\times D}$\n","        - 予測ラベル：$\\boldsymbol{Y}\\in\\mathbb{R}^{N\\times K}$\n","        - 教師ラベル：$\\boldsymbol{T}\\in\\mathbb{R}^{N\\times K}$\n","        - 学習率：$\\epsilon \\in \\mathbb{R}$\n","    - 更新：\n","        - 重みとバイアス $\\boldsymbol{W}\\,\\boldsymbol{b}$\n","        \n","- 勾配降下法: （$\\boldsymbol{W},\\boldsymbol{b}$：パラメータ，$\\epsilon$：学習率）\n","$$\n","\\boldsymbol{W}\\leftarrow\\boldsymbol{W}-\\epsilon\\nabla_{\\boldsymbol{W}}L\\\\\n","\\boldsymbol{b}\\leftarrow\\boldsymbol{b}-\\epsilon\\nabla_{\\boldsymbol{b}}L\n","$$\n","- ソフトマックス回帰の勾配 :\n","\\begin{align}\n","\\nabla_{\\boldsymbol{W}}L&=\\frac{1}{N}\\boldsymbol{X}^\\top(\\boldsymbol{Y}-\\boldsymbol{T})\\\\\n","\\nabla_{\\boldsymbol{b}}L&=\\frac{1}{N}(1,1,...,1)(\\boldsymbol{Y}-\\boldsymbol{T})\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"aPfu71cu0sVs"},"source":["<details>\n","<summary>\n","ヒント\n","</summary>\n","<ol>\n","    <li>\n","    \n","行列の積\n","    <ul> \n","        <li>```np.dot()```</li>\n","    </ul></li>\n","    <li>合計\n","    <ul>\n","    <li>```np.sum()```</li>\n","    </ul></li>\n","</ol>\n","</details>"]},{"cell_type":"code","metadata":{"id":"zWPjvYq90sVt"},"source":["class SoftmaxRegression:\n","    def __init__(self, n_in, n_out):\n","        self.W = np.random.uniform(0.08, -0.08, (n_in, n_out)) #勾配の初期化\n","        self.b = np.zeros(n_out) #バイアスの初期化\n","        \n","    def gradient_decent(self, X, Y, T, eps):\n","        batchsize = X.shape[0]\n","        # TODO\n","        self.W = \n","        self.b = \n","        # TODO\n","        \n","    def train(self, x, t, lr):\n","        y = softmax(np.dot(x, self.W) + self.b) #予測\n","        self.gradient_decent(x, y, t, lr) #パラメータの更新\n","        loss = cross_entropy(y, t) #ロスの算出\n","        return y, loss\n","\n","    def test(self, x, t):\n","        y = softmax(np.dot(x, self.W) + self.b) #予測\n","        loss = cross_entropy(y, t) #ロスの算出\n","        return y, loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_rNHRNX0sVv"},"source":["テスト．以下のセルを実行"]},{"cell_type":"code","metadata":{"id":"kdwl9LAo0sVw"},"source":["test_gradient_decent(SoftmaxRegression)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mcwr-bmQ0sV0"},"source":["# 7. 学習\n","## 7.1 モデルの初期化\n","入力は784次元，出力は10次元"]},{"cell_type":"code","metadata":{"id":"q1c4NKEb0sV0"},"source":["model = SoftmaxRegression(784, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKwwvm4k0sV3"},"source":["## 7.2 ハイパーパラメータの設定\n","- 学習epoch数は20\n","    - epoch数とは，学習データを何回学習するかを表す数である．\n","- バッチサイズは100\n","    - ミニバッチとは少数のサンプルからなる集合である．\n","- 学習率は1"]},{"cell_type":"code","metadata":{"id":"LUY-Z1s60sV4"},"source":["n_epoch = 20\n","batchsize = 100\n","lr = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7g0yf9n0sV6"},"source":["## 7.3 学習\n","交差エントロピー誤差を確率的勾配降下法を用いて最小化する．"]},{"cell_type":"code","metadata":{"id":"3hoW6nbq0sV7"},"source":["for epoch in range(n_epoch):\n","    print ('epoch %d |　' % epoch, end=\"\")\n","    \n","    # Training\n","    sum_loss = 0\n","    pred_label = []\n","    perm = np.random.permutation(train_n) #ランダムに並び替える\n","    \n","    for i in range(0, train_n, batchsize): #ミニバッチごとに学習を行う\n","        x = train_x[perm[i:i+batchsize]]\n","        y = train_y[perm[i:i+batchsize]]\n","        \n","        pred, loss = model.train(x, y, lr)\n","        sum_loss += loss * x.shape[0]\n","        # pred には， (N, 10)の形で，画像が0~9の各数字のどれに分類されるかの事後確率が入っている\n","        # そこで，最も大きい値をもつインデックスを取得することで，識別結果を得ることができる\n","        pred_label.extend(pred.argmax(axis=1))\n","\n","    loss = sum_loss / train_n\n","    # 正解率\n","    accu = accuracy_score(pred_label, np.argmax(train_y[perm], axis=1))\n","    print('Train loss %.3f, accuracy %.4f |　' %(loss, accu), end=\"\")\n","    \n","    \n","    # Testing\n","    sum_loss = 0\n","    pred_label = []\n","    \n","    for i in range(0, test_n, batchsize):\n","        x = test_x[i: i+batchsize]\n","        y = test_y[i: i+batchsize]\n","        \n","        pred, loss = model.test(x, y)\n","        sum_loss += loss * x.shape[0]\n","        pred_label.extend(pred.argmax(axis=1))\n","        \n","    loss = sum_loss / test_n\n","    \n","    accu = accuracy_score(pred_label, np.argmax(test_y, axis=1))\n","    print('Test loss %.3f, accuracy %.4f' %(loss, accu) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUSJ15NA0sWB"},"source":["テストの正解率が92%程度になると成功です．"]}]}